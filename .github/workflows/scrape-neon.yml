# Scraper workflow - Runs on GitHub Actions, saves to Neon PostgreSQL
# Free tier: 2000 min/month
name: Run Scrapers (Neon)

on:
  schedule:
    - cron: '0 12 * * 1,3,5'
  workflow_dispatch:
    inputs:
      portals:
        description: 'Portals to scrape (comma-separated)'
        required: false
        default: 'habitaclia,fotocasa,idealista,milanuncios'
      zones:
        description: 'Zones to scrape (comma-separated)'
        required: false
        default: 'salou,cambrils,tarragona'

env:
  DATABASE_URL: ${{ secrets.NEON_DATABASE_URL }}
  TENANT_ID: 1

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Chrome
        run: |
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      - name: Install dependencies
        run: pip install botasaurus psycopg2-binary python-decouple dbt-postgres requests "camoufox[geoip]" Pillow

      - name: Install Camoufox browser
        run: camoufox fetch

      - name: Run Habitaclia scraper
        if: contains(github.event.inputs.portals || 'habitaclia,fotocasa', 'habitaclia')
        run: python run_habitaclia_scraper.py --zones ${{ github.event.inputs.zones || 'salou,cambrils,tarragona' }} --postgres
        env:
          DATABASE_URL: ${{ secrets.NEON_DATABASE_URL }}

      - name: Run Fotocasa scraper
        if: contains(github.event.inputs.portals || 'habitaclia,fotocasa', 'fotocasa')
        run: python run_fotocasa_scraper.py --zones ${{ github.event.inputs.zones || 'salou,cambrils,tarragona' }} --postgres
        env:
          DATABASE_URL: ${{ secrets.NEON_DATABASE_URL }}

      - name: Run Idealista scraper (Camoufox)
        if: contains(github.event.inputs.portals || '', 'idealista')
        run: |
          python run_camoufox_idealista_scraper.py \
            --zones ${{ github.event.inputs.zones || 'salou,cambrils,tarragona' }} \
            --max-pages 2 \
            --postgres
        env:
          DATABASE_URL: ${{ secrets.NEON_DATABASE_URL }}
          DATADOME_PROXY: ${{ secrets.DATADOME_PROXY }}

      - name: Run Milanuncios scraper (Camoufox)
        if: contains(github.event.inputs.portals || '', 'milanuncios')
        run: |
          python run_camoufox_milanuncios_scraper.py \
            --zones ${{ github.event.inputs.zones || 'salou,cambrils,tarragona' }} \
            --max-pages 2 \
            --postgres
        env:
          DATABASE_URL: ${{ secrets.NEON_DATABASE_URL }}

      - name: Run dbt transformations
        run: |
          cd dbt_project
          mkdir -p /tmp/dbt_profiles
          cat > /tmp/dbt_profiles/profiles.yml << EOF
          casa_teva:
            target: prod
            outputs:
              prod:
                type: postgres
                host: ep-ancient-darkness-agqilf7l-pooler.c-2.eu-central-1.aws.neon.tech
                port: 5432
                user: neondb_owner
                password: "$DBT_PASSWORD"
                dbname: neondb
                schema: public
                threads: 4
                sslmode: require
          EOF
          dbt deps --profiles-dir /tmp/dbt_profiles || true
          dbt run --select staging --profiles-dir /tmp/dbt_profiles
          dbt run --select marts --profiles-dir /tmp/dbt_profiles
        env:
          DBT_PASSWORD: ${{ secrets.NEON_DB_PASSWORD }}

      - name: Summary
        run: |
          echo "## Scraping Complete" >> $GITHUB_STEP_SUMMARY
          echo "- Portals: ${{ github.event.inputs.portals || 'habitaclia,fotocasa' }}" >> $GITHUB_STEP_SUMMARY
          echo "- Zones: ${{ github.event.inputs.zones || 'salou,cambrils,tarragona' }}" >> $GITHUB_STEP_SUMMARY
          echo "- dbt: Transformations applied" >> $GITHUB_STEP_SUMMARY

  notify:
    needs: scrape
    runs-on: ubuntu-latest
    if: failure()
    steps:
      - name: Send failure notification
        run: echo "Scraping failed - check workflow logs"
