# Scraper workflow - Runs on GitHub Actions, saves to Neon PostgreSQL
# fotocasa + idealista + milanuncios via IPRoyal proxy, habitaclia runs on VPS
# Free tier: 2000 min/month
name: Run Scrapers (Neon)

on:
  schedule:
    - cron: '0 12 * * 1,3,5'
  workflow_dispatch:
    inputs:
      portals:
        description: 'Portals to scrape (comma-separated)'
        required: false
        default: 'habitaclia,fotocasa,idealista,milanuncios'
      zones:
        description: 'Zones to scrape (comma-separated)'
        required: false
        default: 'salou,cambrils,tarragona'

env:
  DATABASE_URL: ${{ secrets.NEON_DATABASE_URL }}
  TENANT_ID: 1

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Chrome and Xvfb
        run: |
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo gpg --dearmor -o /usr/share/keyrings/google-chrome.gpg
          echo "deb [arch=amd64 signed-by=/usr/share/keyrings/google-chrome.gpg] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable xvfb

      - name: Install dependencies
        run: pip install botasaurus psycopg2-binary python-decouple python-dotenv dbt-postgres requests "camoufox[geoip]" Pillow

      - name: Install Camoufox browser
        run: camoufox fetch

      - name: Run Habitaclia scraper (Camoufox + proxy)
        if: contains(github.event.inputs.portals || 'habitaclia,fotocasa,idealista,milanuncios', 'habitaclia')
        continue-on-error: true
        run: |
          ZONES="${{ github.event.inputs.zones || 'salou,cambrils,tarragona' }}"
          python run_habitaclia_scraper.py --zones ${ZONES//,/ } --postgres
        env:
          DATABASE_URL: ${{ secrets.NEON_DATABASE_URL }}
          DATADOME_PROXY: ${{ secrets.DATADOME_PROXY }}
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}

      - name: Run Fotocasa scraper (Camoufox + proxy)
        if: contains(github.event.inputs.portals || 'habitaclia,fotocasa,idealista,milanuncios', 'fotocasa')
        continue-on-error: true
        run: |
          ZONES="${{ github.event.inputs.zones || 'salou,cambrils,tarragona' }}"
          python run_camoufox_fotocasa_scraper.py \
            --zones ${ZONES//,/ } \
            --virtual \
            --postgres
        env:
          DATABASE_URL: ${{ secrets.NEON_DATABASE_URL }}
          CAPTCHA_API_KEY: ${{ secrets.CAPTCHA_API_KEY }}
          DATADOME_PROXY: ${{ secrets.DATADOME_PROXY }}
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}

      - name: Run Milanuncios scraper (Camoufox + proxy)
        if: contains(github.event.inputs.portals || 'habitaclia,fotocasa,idealista,milanuncios', 'milanuncios')
        continue-on-error: true
        run: |
          ZONES="${{ github.event.inputs.zones || 'salou,cambrils,tarragona' }}"
          python run_camoufox_milanuncios_scraper.py \
            --zones ${ZONES//,/ } \
            --max-pages 2 \
            --postgres
        env:
          DATABASE_URL: ${{ secrets.NEON_DATABASE_URL }}
          DATADOME_PROXY: ${{ secrets.DATADOME_PROXY }}
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}

      - name: Run Idealista scraper (Camoufox + Xvfb + proxy)
        if: contains(github.event.inputs.portals || 'habitaclia,fotocasa,idealista,milanuncios', 'idealista')
        continue-on-error: true
        run: |
          ZONES="${{ github.event.inputs.zones || 'salou,cambrils,tarragona' }}"
          python run_camoufox_idealista_scraper.py \
            --zones ${ZONES//,/ } \
            --max-pages 2 \
            --virtual \
            --postgres
        env:
          DATABASE_URL: ${{ secrets.NEON_DATABASE_URL }}
          DATADOME_PROXY: ${{ secrets.DATADOME_PROXY }}
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}

      - name: Upload debug HTML (if any)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-html
          path: output/debug_*.html
          if-no-files-found: ignore
          retention-days: 3

      - name: Run dbt transformations
        run: |
          cd dbt_project
          mkdir -p /tmp/dbt_profiles
          cat > /tmp/dbt_profiles/profiles.yml << EOF
          casa_teva:
            target: prod
            outputs:
              prod:
                type: postgres
                host: ep-ancient-darkness-agqilf7l-pooler.c-2.eu-central-1.aws.neon.tech
                port: 5432
                user: neondb_owner
                password: "$DBT_PASSWORD"
                dbname: neondb
                schema: public
                threads: 4
                sslmode: require
          EOF
          dbt deps --profiles-dir /tmp/dbt_profiles || true
          dbt run --select staging --profiles-dir /tmp/dbt_profiles
          dbt run --select marts --profiles-dir /tmp/dbt_profiles
        env:
          DBT_PASSWORD: ${{ secrets.NEON_DB_PASSWORD }}

      - name: Check removed listings (habitaclia + fotocasa)
        continue-on-error: true
        run: python -m scrapers.listing_checker --limit 200
        env:
          DATABASE_URL: ${{ secrets.NEON_DATABASE_URL }}
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}

      - name: Auto-queue new leads for contact
        run: python scripts/post_scrape_auto_queue.py
        env:
          DATABASE_URL: ${{ secrets.NEON_DATABASE_URL }}
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}

      - name: Validate scrape quality
        continue-on-error: true
        run: python scripts/validate_scrape_quality.py
        env:
          DATABASE_URL: ${{ secrets.NEON_DATABASE_URL }}
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}

      - name: Summary
        run: |
          echo "## Scraping Complete" >> $GITHUB_STEP_SUMMARY
          echo "- Portals: ${{ github.event.inputs.portals || 'habitaclia,fotocasa,idealista,milanuncios' }}" >> $GITHUB_STEP_SUMMARY
          echo "- Zones: ${{ github.event.inputs.zones || 'salou,cambrils,tarragona' }}" >> $GITHUB_STEP_SUMMARY
          echo "- dbt: Transformations applied" >> $GITHUB_STEP_SUMMARY

  notify:
    needs: scrape
    runs-on: ubuntu-latest
    if: failure()
    steps:
      - name: Send failure notification via Telegram
        run: |
          curl -s -X POST "https://api.telegram.org/bot${TELEGRAM_BOT_TOKEN}/sendMessage" \
            -d chat_id="${TELEGRAM_CHAT_ID}" \
            -d parse_mode=HTML \
            -d text="ðŸ”´ <b>GH Actions Scraping FAILED</b>%0A%0AWorkflow: scrape-neon.yml%0ACheck: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
        env:
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}
